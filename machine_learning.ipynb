{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VA6VjLaqTbwe",
        "outputId": "97817147-46d0-4859-ebde-e2137f5bd3d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras import layers\n",
        "from keras.callbacks import Callback, ModelCheckpoint, ReduceLROnPlateau\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import pandas as pd\n",
        "import scipy\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "from collections import Counter\n",
        "import json\n",
        "import itertools\n",
        "from keras.layers import Dense,Dropout,Activation,Add,MaxPooling2D,Conv2D,Flatten,BatchNormalization\n",
        "import os\n",
        "from PIL import Image\n",
        "import cv2\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, roc_curve,roc_auc_score, auc\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "wFrOqtp5TgYK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Gathering and Preprocessing"
      ],
      "metadata": {
        "id": "JZSPQFZuTvYe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create data generator\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    zoom_range=0.1,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "# Define the data generator for the validation and test sets\n",
        "val_test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# fit the model on data\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    '/content/drive/MyDrive/Group_Project_Data 2/Group_Project_Data/Train',\n",
        "    target_size=(64,64),\n",
        "    batch_size=32,\n",
        "    class_mode='binary')\n",
        "    \n",
        "validation_generator = val_test_datagen.flow_from_directory(\n",
        "    '/content/drive/MyDrive/Group_Project_Data 2/Group_Project_Data/Valid',\n",
        "    target_size=(64,64),\n",
        "    batch_size=32,\n",
        "    class_mode='binary')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TgRQragaTzLQ",
        "outputId": "57211068-97c4-4024-d70f-d08c23bd621f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 6000 images belonging to 2 classes.\n",
            "Found 2000 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def Dataset_loader(DIR, RESIZE, sigmaX=10):\n",
        "    IMG = []\n",
        "    read = lambda imname: np.asarray(Image.open(imname).convert(\"RGB\"))\n",
        "    \n",
        "    for root, _, files in os.walk(DIR):\n",
        "        for file in files:\n",
        "            if file.endswith('.png'):\n",
        "                PATH = os.path.join(root,file)\n",
        "\n",
        "                img = read(PATH)\n",
        "\n",
        "                img = cv2.resize(img, (RESIZE,RESIZE))\n",
        "\n",
        "                IMG.append(np.array(img))\n",
        "    return IMG\n",
        "\n",
        "real_train = np.array(Dataset_loader('/content/drive/MyDrive/Group_Project_Data 2/Group_Project_Data/Train/Real',64))\n",
        "fake_train = np.array(Dataset_loader('/content/drive/MyDrive/Group_Project_Data 2/Group_Project_Data/Train/Fake',64))\n",
        "real_test = np.array(Dataset_loader('/content/drive/MyDrive/Group_Project_Data 2/Group_Project_Data/Valid/Real',64))\n",
        "fake_test = np.array(Dataset_loader('/content/drive/MyDrive/Group_Project_Data 2/Group_Project_Data/Valid/Fake',64))"
      ],
      "metadata": {
        "id": "RqqEKFSXUeiY"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split dataset"
      ],
      "metadata": {
        "id": "4pQo9TrCUk9r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create labels\n",
        "real_train_label = np.ones(len(real_train))\n",
        "fake_train_label = np.zeros(len(fake_train))\n",
        "real_test_label = np.ones(len(real_test))\n",
        "fake_test_label = np.zeros(len(fake_test))\n",
        "\n",
        "# Merge data \n",
        "X_train = np.concatenate((real_train, fake_train), axis = 0)\n",
        "Y_train = np.concatenate((real_train_label, fake_train_label), axis = 0)\n",
        "X_test = np.concatenate((real_test, fake_test), axis = 0)\n",
        "Y_test = np.concatenate((real_test_label, fake_test_label), axis = 0)\n",
        "\n",
        "# Shuffle train data\n",
        "s = np.arange(X_train.shape[0])\n",
        "np.random.shuffle(s)\n",
        "X_train = X_train[s]\n",
        "Y_train = Y_train[s]\n",
        "\n",
        "# Shuffle test data\n",
        "s = np.arange(X_test.shape[0])\n",
        "np.random.shuffle(s)\n",
        "X_test = X_test[s]\n",
        "Y_test = Y_test[s]\n"
      ],
      "metadata": {
        "id": "VrqchUCVUm_G"
      },
      "execution_count": 10,
      "outputs": []
    }
  ]
}